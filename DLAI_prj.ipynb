{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBD9md8vGChJ"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "### Code attributions\n",
        "\n",
        "Pytorch geometric tutorials: https://pytorch-geometric.readthedocs.io/en/latest/notes/colabs.html\n",
        "\n",
        "PyGCL: Although I have ended up not using their code, I still read their article here: https://sxkdz.github.io/research/GraphCL/.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITDEMxF0GF6C"
      },
      "source": [
        "# Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKJjGtp1zuPO"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {
          "iopub.execute_input": "2022-09-01T09:44:23.116935Z",
          "iopub.status.busy": "2022-09-01T09:44:23.115317Z",
          "iopub.status.idle": "2022-09-01T09:44:50.261323Z",
          "shell.execute_reply": "2022-09-01T09:44:50.260035Z",
          "shell.execute_reply.started": "2022-09-01T09:44:23.116777Z"
        },
        "id": "cOGtFZwJOHmb"
      },
      "outputs": [],
      "source": [
        "#@title Dependencies and utility functions\n",
        "#See above for the source\n",
        "\n",
        "import os\n",
        "import itertools\n",
        "import torch\n",
        "import copy\n",
        "from tqdm.notebook import tqdm\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "print(torch.__version__)\n",
        "\n",
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q torch-cluster -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "torch.manual_seed(12345)\n",
        "\n",
        "#Visualization methods for 3d shapes and point clouds\n",
        "def visualize_mesh(pos, face):\n",
        "    fig = plt.figure()\n",
        "    ax = fig.gca(projection='3d')\n",
        "    ax.axes.xaxis.set_ticklabels([])\n",
        "    ax.axes.yaxis.set_ticklabels([])\n",
        "    ax.axes.zaxis.set_ticklabels([])\n",
        "    ax.plot_trisurf(pos[:, 0], pos[:, 1], pos[:, 2], triangles=face.t(), antialiased=False)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def visualize_points(pos, edge_index=None, index=None):\n",
        "    fig = plt.figure(figsize=(4, 4))\n",
        "    if edge_index is not None:\n",
        "        for (src, dst) in edge_index.t().tolist():\n",
        "             src = pos[src].tolist()\n",
        "             dst = pos[dst].tolist()\n",
        "             plt.plot([src[0], dst[0]], [src[1], dst[1]], linewidth=1, color='black')\n",
        "    if index is None:\n",
        "        plt.scatter(pos[:, 0], pos[:, 1], s=50, zorder=1000)\n",
        "    else:\n",
        "       mask = torch.zeros(pos.size(0), dtype=torch.bool)\n",
        "       mask[index] = True\n",
        "       plt.scatter(pos[~mask, 0], pos[~mask, 1], s=50, color='lightgray', zorder=1000)\n",
        "       plt.scatter(pos[mask, 0], pos[mask, 1], s=50, zorder=1000)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "    \n",
        "\n",
        "def get_optimizer(opt_name, model, lr):\n",
        "  #There is no match in python 3.7, which is the version colab uses :/\n",
        "    if opt_name == \"Adam\":\n",
        "        return torch.optim.Adam(model.parameters(), lr)\n",
        "    elif opt_name == \"SGD\":\n",
        "        return torch.optim.SGD(model.parameters(), lr, momentum=1e-4,\n",
        "                           dampening=1e-6)\n",
        "    return None\n",
        "\n",
        "\n",
        "def get_loss_by_name(loss_f_name):\n",
        "    if loss_f_name == \"CELoss\":\n",
        "        return torch.nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {
          "iopub.execute_input": "2022-08-25T14:20:34.579281Z",
          "iopub.status.busy": "2022-08-25T14:20:34.578315Z",
          "iopub.status.idle": "2022-08-25T14:21:33.881910Z",
          "shell.execute_reply": "2022-08-25T14:21:33.880448Z",
          "shell.execute_reply.started": "2022-08-25T14:20:34.579201Z"
        },
        "id": "h9mFuIv8ZGWR"
      },
      "outputs": [],
      "source": [
        "#@title Remove existing data\n",
        "!rm -rf training_data_1\n",
        "!rm -rf test_data_1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-01T09:44:50.264056Z",
          "iopub.status.busy": "2022-09-01T09:44:50.263599Z",
          "iopub.status.idle": "2022-09-01T09:46:36.858080Z",
          "shell.execute_reply": "2022-09-01T09:46:36.857302Z",
          "shell.execute_reply.started": "2022-09-01T09:44:50.264028Z"
        },
        "id": "fsZElEWmTSCh"
      },
      "outputs": [],
      "source": [
        "#Extend dataset with augmented samples\n",
        "\n",
        "from torch_geometric.data import Dataset\n",
        "from torch_geometric.datasets import GeometricShapes, ModelNet\n",
        "from torch_geometric.transforms import RandomRotate, Compose, SamplePoints, ToDevice\n",
        "from torch_geometric.loader import DataLoader\n",
        "from collections import defaultdict\n",
        "\n",
        "def shrink_ModelNet(dataset, max):\n",
        "    \n",
        "    datalist = []\n",
        "    num_classes = defaultdict(lambda: 0)\n",
        "    for data in dataset:\n",
        "        class_num = int(data[0].y)\n",
        "        if num_classes[class_num] < max:\n",
        "            datalist.append(data)\n",
        "            num_classes[class_num] += 1\n",
        "    \n",
        "    return datalist\n",
        "  \n",
        "\n",
        "#Extend existing Class\n",
        "class AugmentedDS(Dataset):\n",
        "\n",
        "  \"\"\"An augmented version of the GeometricShapes dataset\"\"\"\n",
        "\n",
        "  def __init__(self, root: str, train:bool, augmentor, transformer, ds_name: str):\n",
        "        \n",
        "        if ds_name == \"GeometricShapes\":\n",
        "            self.dataset = GeometricShapes(root = root, train = train)\n",
        "        else:\n",
        "            self.dataset = ModelNet(root=root, train=train, name='10')\n",
        "        self.transformer = transformer\n",
        "        self.augmentor_transformer = Compose([augmentor,transformer])\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.dataset)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "\n",
        "    original_shape = self.dataset[idx]\n",
        "    augmented_shape1 = self.augmentor_transformer(original_shape.clone()) \n",
        "    augmented_shape2 = self.augmentor_transformer(original_shape.clone())\n",
        "    augmented_shape3 = self.augmentor_transformer(original_shape.clone()) \n",
        "    original_shape = self.transformer(original_shape)\n",
        "\n",
        "    return original_shape, augmented_shape1, augmented_shape2, augmented_shape3\n",
        "\n",
        "\n",
        "dataset = \"ModelNet\"\n",
        "batch_size = 80 if dataset == \"ModelNet\" else 40\n",
        "nr_points = 1024 if dataset==\"ModelNet\" else 256\n",
        "training_ds_root = \"training_data_1\" if dataset == \"ModelNet\" else \"training_data\"\n",
        "test_ds_root = \"test_data_1\" if dataset == \"ModelNet\" else \"test_data\"\n",
        "print(\"Sampling : {} points\".format(nr_points))\n",
        "\n",
        "#Augmentors and transformers for the data\n",
        "augmentor = Compose([\n",
        "    RandomRotate(degrees=90, axis=0),\n",
        "    RandomRotate(degrees=90, axis=1),\n",
        "    RandomRotate(degrees=90, axis=2)\n",
        "])\n",
        "\n",
        "transformer=Compose([\n",
        "    SamplePoints(num=nr_points, include_normals=True),\n",
        "    ToDevice(device)\n",
        "])\n",
        "\n",
        "\n",
        "train_ds = AugmentedDS(root = training_ds_root, augmentor=augmentor,\n",
        "                       transformer=transformer, train=True, ds_name = dataset)\n",
        "test_ds = AugmentedDS(root = test_ds_root, augmentor=augmentor,\n",
        "                       transformer=transformer, train=False, ds_name = dataset)\n",
        "\n",
        "#Shrink dataset if it's modelnet\n",
        "if dataset == \"ModelNet\":\n",
        "    datalist_train = shrink_ModelNet(train_ds, 81)\n",
        "    datalist_train2 = shrink_ModelNet(train_ds, 10)\n",
        "    datalist_test = shrink_ModelNet(test_ds, 31)\n",
        "\n",
        "    train_dl = DataLoader(datalist_train, batch_size = batch_size, shuffle=True)\n",
        "    #use 2nd datalist to train with less labels\n",
        "    #train_dl2 = DataLoader(datalist_train2, batch_size = batch_size, shuffle=True)\n",
        "    test_dl = DataLoader(datalist_test, batch_size = batch_size, shuffle=True)\n",
        "else:\n",
        "    train_dl = DataLoader(train_ds, batch_size = batch_size, shuffle=True)\n",
        "    test_dl = DataLoader(test_ds, batch_size = batch_size, shuffle=True) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-01T09:46:36.859933Z",
          "iopub.status.busy": "2022-09-01T09:46:36.859670Z",
          "iopub.status.idle": "2022-09-01T09:46:36.873591Z",
          "shell.execute_reply": "2022-09-01T09:46:36.872881Z",
          "shell.execute_reply.started": "2022-09-01T09:46:36.859909Z"
        },
        "id": "PWlVqrano-aJ"
      },
      "outputs": [],
      "source": [
        "#@title Print informations about the dataset\n",
        "if dataset == \"ModelNet\":\n",
        "    print(\"Dataset length: \", len(datalist_train), len(datalist_test), \" \\nDataset keys: \", train_ds[0][0].keys)\n",
        "else:\n",
        "    print(\"Dataset length: \", len(train_ds), len(test_ds), \" \\nDataset keys: \", train_ds[0][0].keys)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5YSUH7Zzy5L"
      },
      "source": [
        "## Contrastive learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-01T09:46:36.876032Z",
          "iopub.status.busy": "2022-09-01T09:46:36.875396Z",
          "iopub.status.idle": "2022-09-01T09:46:37.071962Z",
          "shell.execute_reply": "2022-09-01T09:46:37.071014Z",
          "shell.execute_reply.started": "2022-09-01T09:46:36.876006Z"
        },
        "id": "76GD6Kcgb4Pt"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.nn import PPFConv, global_max_pool\n",
        "from torch.nn import Sequential, Linear, ReLU\n",
        "from torch_cluster import fps, knn_graph\n",
        "\n",
        "class PPFNet(torch.nn.Module):\n",
        "    def __init__(self, feat_dim, hidden_layer_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        #Dimension of hidden layers\n",
        "        self.hidden_layer_dim = hidden_layer_dim\n",
        "        #Dimension of feature, 3 with 3D fd and 4 with 4D\n",
        "        self.feat_dim = feat_dim\n",
        "\n",
        "        \n",
        "        mlp1 = Sequential(Linear(self.feat_dim, self.hidden_layer_dim),\n",
        "                              ReLU(),\n",
        "                              Linear(self.hidden_layer_dim, self.hidden_layer_dim*2))\n",
        "        self.conv1 = PPFConv(local_nn = mlp1)  \n",
        "\n",
        "        \n",
        "        mlp2 = Sequential(Linear(self.hidden_layer_dim*2 + self.feat_dim, self.hidden_layer_dim*2 + self.feat_dim),\n",
        "                              ReLU(),\n",
        "                              Linear(self.hidden_layer_dim*2 + self.feat_dim, self.hidden_layer_dim*2 + self.feat_dim))  \n",
        "        self.conv2 = PPFConv(local_nn = mlp2)  \n",
        "        \n",
        "        self.prj_head = Sequential(\n",
        "            Linear(self.hidden_layer_dim*2 + self.feat_dim, 100),\n",
        "            ReLU(),\n",
        "            Linear(100,100)\n",
        "        )\n",
        "        \n",
        "    def forward(self, pos, normal, batch):\n",
        "\n",
        "        #Create edges in the point cloud\n",
        "        edge_index = knn_graph(pos, k=16, batch=batch, loop=False)\n",
        "        \n",
        "        #There are no features in first convolution\n",
        "        #Other datasets different from GS or MN may have them\n",
        "        x = self.conv1(x=None, pos=pos, normal=normal, edge_index=edge_index)\n",
        "        x = x.relu()\n",
        "        \n",
        "        #farthest point sampling\n",
        "        index = fps(pos, batch, ratio=0.5)\n",
        "        x = x[index]\n",
        "        pos = pos[index]\n",
        "        normal = normal[index]\n",
        "        batch = batch[index]\n",
        "        edge_index = knn_graph(pos, k=16, batch=batch, loop=False)\n",
        "        \n",
        "        x = self.conv2(x=x, pos=pos, normal=normal, edge_index=edge_index)\n",
        "        x = x.relu()\n",
        "        \n",
        "        x = global_max_pool(x, batch)  # [num_examples, hidden_channels]\n",
        "        return self.prj_head(x)\n",
        "\n",
        "\n",
        "model = PPFNet(feat_dim=4, hidden_layer_dim = 32)\n",
        "model.to(device)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-01T09:46:37.073338Z",
          "iopub.status.busy": "2022-09-01T09:46:37.073069Z",
          "iopub.status.idle": "2022-09-01T09:46:37.093720Z",
          "shell.execute_reply": "2022-09-01T09:46:37.092168Z",
          "shell.execute_reply.started": "2022-09-01T09:46:37.073315Z"
        },
        "id": "Fqro1ljmkrZZ"
      },
      "outputs": [],
      "source": [
        "#@title Contrastive training\n",
        "\n",
        "from torch_geometric.utils import to_dense_batch, to_dense_adj\n",
        "from torch.nn import CosineSimilarity\n",
        "from torch.optim.lr_scheduler import LinearLR \n",
        "from random import randint\n",
        "\n",
        "cos = CosineSimilarity(dim=0)\n",
        "cos2 = CosineSimilarity(dim=1)\n",
        "\n",
        "def InfoNCELossSN(anchors, augmented, temperature=0.05):\n",
        "\n",
        "  loss = 0\n",
        "  batch_len = anchors.shape[0]\n",
        "\n",
        "  for index in range(batch_len):\n",
        "    anchor = anchors[index]\n",
        "    pos_sample = augmented[index]\n",
        "    pos_sim = torch.exp(cos(anchor, pos_sample) / temperature)\n",
        "    neg_sim = 0\n",
        "    \n",
        "    if index == 0:\n",
        "        negatives = augmented[index+1:, :]\n",
        "    elif index == batch_len-1:\n",
        "        negatives = augmented[:index, :]\n",
        "    else:\n",
        "        negatives = torch.cat((augmented[:index, :], augmented[index+1:, :]), dim=0)\n",
        "    \n",
        "    neg_sim = torch.einsum(\"i-> \", torch.exp(cos2(anchor, negatives) / temperature))\n",
        "        \n",
        "    loss += -torch.log(pos_sim / (pos_sim + neg_sim))\n",
        "    \n",
        "  return loss \n",
        "\n",
        "def InfoNCELoss(anchors, augmented1, augmented2, temperature=0.05):\n",
        "\n",
        "  \"\"\"InfoNCE with support to multiple positives\"\"\"\n",
        "  loss = 0\n",
        "  batch_len = anchors.shape[0]\n",
        "\n",
        "  for index in range(batch_len):\n",
        "    anchor = anchors[index]\n",
        "    positives = (torch.cat((augmented1[index], augmented2[index]), dim=0)).reshape(2, -1)\n",
        "    pos_sim = torch.einsum(\"i->\",torch.exp(cos2(anchor, positives) / temperature))\n",
        "    \n",
        "    neg_sim = 0\n",
        "    #Handle first and last row\n",
        "    if index == 0:\n",
        "        negatives = torch.cat((augmented1[index+1:, :], augmented2[index+1:, :]), dim=0)\n",
        "    elif index == batch_len-1:\n",
        "        negatives = torch.cat((augmented1[:index, :],augmented2[:index, :]),dim=0)\n",
        "    else:\n",
        "        n1 = torch.cat((augmented1[:index, :], augmented1[index+1:, :]), dim=0)\n",
        "        n2 = torch.cat((augmented2[:index, :], augmented2[index+1:, :]), dim=0)\n",
        "        negatives = torch.cat((n1, n2), dim=0)\n",
        "         \n",
        "    neg_sim = torch.einsum(\"i-> \", torch.exp(cos2(anchor, negatives) / temperature))\n",
        "    \n",
        "    loss += -torch.log(pos_sim / (pos_sim + neg_sim))\n",
        "\n",
        "  return loss \n",
        "\n",
        "def train(model, optim, loader, temperature, scheduler=None):\n",
        "    model.train()\n",
        "\n",
        "    total_loss = 0\n",
        "    for data in loader:\n",
        "        optim.zero_grad()  # Clear gradients.\n",
        "        \n",
        "        #random_n = randint(1, 3)\n",
        "        #choose one of the n augmentations\n",
        "        original, augmented1, augmented2 = data[0], data[1], data[2]\n",
        "        \n",
        "        z1 = model(original.pos, original.normal, original.batch)\n",
        "        z2 = model(augmented1.pos, augmented1.normal, augmented1.batch)\n",
        "        z3 = model(augmented2.pos, augmented2.normal, augmented2.batch)\n",
        "        #print(z1.shape, z2.shape)\n",
        "\n",
        "        loss = InfoNCELoss(anchors=z1, augmented1=z2, augmented2=z3, \n",
        "                            temperature=temperature)\n",
        "        #loss = InfoNCELossSN(anchors=z1, augmented=z2, temperature=temperature)\n",
        "                            \n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "        total_loss += loss\n",
        "\n",
        "    scheduler.step() if scheduler is not None else None\n",
        "    return total_loss \n",
        "\n",
        "def train_contrastive(model, hyperparams_dict, debug=True):\n",
        "\n",
        "  \"\"\"Expects a dictionary containing the parameters to test.\"\"\"\n",
        "  for params in tqdm(hyperparams_dict):\n",
        "    \n",
        "    lr, nr_epochs, optim_name, temperature = params[0], params[1], params[2], params[3]\n",
        "    optim = get_optimizer(opt_name = optim_name, model = model, lr=lr)\n",
        "    \n",
        "    best_acc = 10000\n",
        "    for epoch in range(1, nr_epochs):\n",
        "        loss = train(model, optim, train_dl,\n",
        "                     temperature=temperature)\n",
        "        \n",
        "        #break\n",
        "        \n",
        "        if debug and (epoch % 10 == 0):\n",
        "              print(\"Epoch\", epoch, \"loss\", loss)\n",
        "        best_acc = loss if loss < best_acc else best_acc\n",
        "\n",
        "    result = \"Best loss of {} achieved with the following hyperparams: lr {} \\t #epochs {} \\t optim {} \\t temperature {}\".format(best_acc, lr, nr_epochs, \n",
        "                                                                                                                                     optim_name, temperature)\n",
        "    #break\n",
        "    print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-01T09:46:37.096653Z",
          "iopub.status.busy": "2022-09-01T09:46:37.096141Z",
          "iopub.status.idle": "2022-09-01T10:05:41.352758Z",
          "shell.execute_reply": "2022-09-01T10:05:41.351669Z",
          "shell.execute_reply.started": "2022-09-01T09:46:37.096603Z"
        },
        "id": "P8n_sHfNksXm"
      },
      "outputs": [],
      "source": [
        "#Hypeparams\n",
        "lr_dict = {0.01}\n",
        "epochs = {150}\n",
        "#use 0.3 for modelnet, 0.02 for geom shapes\n",
        "temperature = {0.03}\n",
        "optim_dict = {\"Adam\"} \n",
        "\n",
        "hyperparams_dict = itertools.product(lr_dict, epochs, optim_dict, temperature)\n",
        "train_contrastive(model, hyperparams_dict) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-08-29T09:34:52.760398Z",
          "iopub.status.busy": "2022-08-29T09:34:52.760160Z",
          "iopub.status.idle": "2022-08-29T09:34:52.771218Z",
          "shell.execute_reply": "2022-08-29T09:34:52.770487Z",
          "shell.execute_reply.started": "2022-08-29T09:34:52.760378Z"
        },
        "id": "u4ethzsA1bUw"
      },
      "outputs": [],
      "source": [
        "model = torch.load(\"Model_updated/Model1024\").to(device)\n",
        "print(model)\n",
        "#torch.save(model, \"Model_updated/Model256\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-gRdXyEz5zp"
      },
      "source": [
        "## Testing results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-01T10:05:41.356706Z",
          "iopub.status.busy": "2022-09-01T10:05:41.356399Z",
          "iopub.status.idle": "2022-09-01T10:05:41.369430Z",
          "shell.execute_reply": "2022-09-01T10:05:41.368302Z",
          "shell.execute_reply.started": "2022-09-01T10:05:41.356681Z"
        },
        "id": "b46c1juSmGVj"
      },
      "outputs": [],
      "source": [
        "#@title Utily functions to test the result of contrastive training\n",
        "\n",
        "class SimpleClassifier(torch.nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    self.mlp = torch.nn.Linear(in_features=100, out_features=40)\n",
        "    self.activation = torch.nn.ReLU()\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.activation(self.mlp(x))\n",
        "\n",
        "\n",
        "def train(model, classifier, optim, loader,\n",
        "          loss_f, scheduler=None):\n",
        "    classifier.train()\n",
        "    \n",
        "    total_loss = 0\n",
        "    for data in loader:\n",
        "\n",
        "        optim.zero_grad()  # Clear gradients.\n",
        "        original = data[0]\n",
        "        z1 = model(original.pos, original.normal, original.batch)\n",
        "        \n",
        "        logits = classifier(z1)\n",
        "        loss = loss_f(logits, data[0].y)\n",
        "        loss.backward()  # Backward pass.\n",
        "        \n",
        "        optim.step()  # Update model parameters.\n",
        "        total_loss += loss\n",
        "\n",
        "    scheduler.step() if scheduler is not None else None\n",
        "    return total_loss / len(train_dl.dataset)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def test(model, classifier, loader):\n",
        "    classifier.eval()\n",
        "\n",
        "    total_correct = 0\n",
        "    for data in loader:\n",
        "\n",
        "        original = data[0]\n",
        "        z1 = model(original.pos, original.normal, original.batch)\n",
        "        logits = classifier(z1)\n",
        "        preds = torch.argmax(logits, dim=-1)\n",
        "        total_correct += int((preds == data[0].y).sum())\n",
        "\n",
        "    return total_correct/(len(loader.dataset))\n",
        "\n",
        "def test_hyperparams(model, hyperparams_dict, debug=True):\n",
        "\n",
        "  \"\"\"This is nothing more than a GridSearch. \n",
        "  It expects a dictionary containing the parameters to test.\"\"\"\n",
        "  for params in tqdm(hyperparams_dict):\n",
        "    \n",
        "    #Clone the classifier to avoid having to create a new one each loop\n",
        "    classifier_test = SimpleClassifier().to(device)\n",
        "    lr, nr_epochs, optim_name, loss_f_name = params[0], params[1], params[2], params[3]\n",
        "    optim = get_optimizer(opt_name = optim_name, model = classifier_test,\n",
        "                        lr=lr)\n",
        "    loss_f = get_loss_by_name(loss_f_name)  \n",
        "\n",
        "    best_acc = 0\n",
        "    for epoch in range(1, nr_epochs):\n",
        "        loss = train(model, classifier_test, optim, train_dl, loss_f)\n",
        "        test_acc = test(model, classifier_test, test_dl)\n",
        "\n",
        "        if debug and (epoch % 50 == 0):\n",
        "          print(\"Epoch\", epoch, \"loss\", loss, \"test_acc\", test_acc)\n",
        "\n",
        "        best_acc = test_acc if test_acc > best_acc else best_acc\n",
        "\n",
        "    result = \"Best accuracy of {} achieved with the following hyperparams: lr {} \\t #epochs {} \\t optim {} \\t loss_f {}\".format(best_acc, lr, nr_epochs,\n",
        "                                                                                                                       optim_name, loss_f_name)\n",
        "    print(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-01T10:05:41.371472Z",
          "iopub.status.busy": "2022-09-01T10:05:41.371161Z",
          "iopub.status.idle": "2022-09-01T13:00:16.729772Z",
          "shell.execute_reply": "2022-09-01T13:00:16.728887Z",
          "shell.execute_reply.started": "2022-09-01T10:05:41.371442Z"
        },
        "id": "WrtI_D9_nlNO"
      },
      "outputs": [],
      "source": [
        "#the model is the previously trained contrastive model\n",
        "#Uncomment the following line to remove projection head\n",
        "#Also switch in_features from 100 to 68\n",
        "#model.prj_head = torch.nn.Identity()\n",
        "loss_f_dict = {\"CELoss\"}\n",
        "lr_dict = {0.01, 0.02}\n",
        "epochs = {100, 250, 500, 1000}\n",
        "optim_dict = {\"Adam\"} \n",
        "\n",
        "hyperparams_dict = itertools.product(lr_dict, epochs, optim_dict, loss_f_dict)\n",
        "results = test_hyperparams(model, hyperparams_dict) \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsexgU_ejs6k"
      },
      "source": [
        "# Standard pointnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-01T13:22:18.764250Z",
          "iopub.status.busy": "2022-09-01T13:22:18.763199Z",
          "iopub.status.idle": "2022-09-01T13:22:18.774060Z",
          "shell.execute_reply": "2022-09-01T13:22:18.773090Z",
          "shell.execute_reply.started": "2022-09-01T13:22:18.764212Z"
        },
        "id": "rnD75ge0js6l"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.nn import PPFConv, global_max_pool\n",
        "from torch.nn import Sequential, Linear, ReLU\n",
        "from torch_cluster import fps, knn_graph\n",
        "\n",
        "class PPFNet(torch.nn.Module):\n",
        "    def __init__(self, feat_dim, hidden_layer_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        #Dimension of hidden layers\n",
        "        self.hidden_layer_dim = hidden_layer_dim\n",
        "        #Dimension of feature, 3 with 3D fd and 4 with 4D\n",
        "        self.feat_dim = feat_dim\n",
        "\n",
        "        \n",
        "        mlp1 = Sequential(Linear(self.feat_dim, self.hidden_layer_dim),\n",
        "                              ReLU(),\n",
        "                              Linear(self.hidden_layer_dim, self.hidden_layer_dim*2))\n",
        "        self.conv1 = PPFConv(local_nn = mlp1)  \n",
        "        \n",
        "        mlp2 = Sequential(Linear(self.hidden_layer_dim*2 + self.feat_dim, self.hidden_layer_dim*2 + self.feat_dim),\n",
        "                              ReLU(),\n",
        "                              Linear(self.hidden_layer_dim*2 + self.feat_dim, self.hidden_layer_dim*2 + self.feat_dim))  \n",
        "        self.conv2 = PPFConv(local_nn = mlp2)  \n",
        "        self.classifier = Linear(68, 40)\n",
        "        \n",
        "        \n",
        "    def forward(self, pos, normal, batch):\n",
        "\n",
        "        #Create edges in the point cloud\n",
        "        edge_index = knn_graph(pos, k=16, batch=batch, loop=False)\n",
        "        \n",
        "        #There are no features in first convolution except the 3D/4D descriptor, \n",
        "        #which is concatenated om the message passing\n",
        "        x = self.conv1(x=None, pos=pos, normal=normal, edge_index=edge_index)\n",
        "        x = x.relu()\n",
        "        \n",
        "        #farthest point sampling\n",
        "        index = fps(pos, batch, ratio=0.5)\n",
        "        x = x[index]\n",
        "        pos = pos[index]\n",
        "        normal = normal[index]\n",
        "        batch = batch[index]\n",
        "        edge_index = knn_graph(pos, k=16, batch=batch, loop=False)\n",
        "        \n",
        "        x = self.conv2(x=x, pos=pos, normal=normal, edge_index=edge_index)\n",
        "        x = x.relu()\n",
        "\n",
        "        x = global_max_pool(x, batch)  # [num_examples, hidden_channels]\n",
        "\n",
        "        return self.classifier(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-01T13:22:20.212515Z",
          "iopub.status.busy": "2022-09-01T13:22:20.211765Z",
          "iopub.status.idle": "2022-09-01T13:22:20.223045Z",
          "shell.execute_reply": "2022-09-01T13:22:20.222182Z",
          "shell.execute_reply.started": "2022-09-01T13:22:20.212485Z"
        },
        "id": "gSo3tGmXjs6l"
      },
      "outputs": [],
      "source": [
        "#@title Standard training\n",
        "\n",
        "from torch_geometric.utils import to_dense_batch, to_dense_adj\n",
        "from torch.nn import CosineSimilarity\n",
        "from torch.optim.lr_scheduler import LinearLR \n",
        "\n",
        "def train(model, optim, loader, loss_f, scheduler=None):\n",
        "    model.train()\n",
        "\n",
        "    total_loss = 0\n",
        "    for data in loader:\n",
        "        optim.zero_grad()  # Clear gradients.\n",
        "    \n",
        "        original= data[0]\n",
        "        logits = model(original.pos, original.normal, original.batch)\n",
        "        loss = loss_f(logits, data[0].y)\n",
        "        loss.backward()  # Backward pass.\n",
        "\n",
        "        optim.step()\n",
        "        total_loss += loss\n",
        "\n",
        "    scheduler.step() if scheduler is not None else None\n",
        "    return total_loss #/batch_size\n",
        "\n",
        "@torch.no_grad()\n",
        "def test(model, loader):\n",
        "    model.eval()\n",
        "\n",
        "    total_correct = 0\n",
        "    for data in loader:\n",
        "\n",
        "        original = data[0]\n",
        "        logits = model(original.pos, original.normal, original.batch)\n",
        "        preds = torch.argmax(logits, dim=-1)\n",
        "        total_correct += int((preds == data[0].y).sum())\n",
        "\n",
        "    return total_correct/(len(loader.dataset))\n",
        "\n",
        "def train_ptnet(hyperparams_dict, debug=True):\n",
        "\n",
        "  \"\"\"This is nothing more than a GridSearch. \n",
        "  It expects a dictionary containing the parameters to test.\"\"\"\n",
        "  for params in tqdm(hyperparams_dict):\n",
        "    \n",
        "    model_st = (PPFNet(feat_dim=4, hidden_layer_dim = 32)).to(device)\n",
        "    lr, nr_epochs, optim_name, loss_f_name = params[0], params[1], params[2], params[3]\n",
        "    optim = get_optimizer(opt_name = optim_name, model = model_st,\n",
        "                        lr=lr)\n",
        "    loss_f = get_loss_by_name(loss_f_name)  \n",
        "\n",
        "    best_acc = 0\n",
        "    for epoch in range(1, nr_epochs):\n",
        "        loss = train(model_st, optim, train_dl, loss_f)\n",
        "        test_acc = test(model_st, test_dl)\n",
        "\n",
        "        if debug and (epoch % 50 == 0):\n",
        "          print(\"Epoch\", epoch, \"loss\", loss, \"test_acc\", test_acc)\n",
        "\n",
        "        best_acc = test_acc if test_acc > best_acc else best_acc\n",
        "\n",
        "    result = \"Best accuracy of {} achieved with the following hyperparams: lr {} \\t #epochs {} \\t optim {} \\t loss_f {}\".format(best_acc, lr, nr_epochs,\n",
        "                                                                                                                       optim_name, loss_f_name)\n",
        "    print(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-01T13:22:21.572772Z",
          "iopub.status.busy": "2022-09-01T13:22:21.572435Z",
          "iopub.status.idle": "2022-09-01T14:41:52.779943Z",
          "shell.execute_reply": "2022-09-01T14:41:52.779307Z",
          "shell.execute_reply.started": "2022-09-01T13:22:21.572746Z"
        },
        "id": "_Elpwc0Rjs6l"
      },
      "outputs": [],
      "source": [
        "loss_f_dict = {\"CELoss\"}\n",
        "lr_dict = {0.01, 0.02}\n",
        "epochs = {100, 250, 500}\n",
        "optim_dict = {\"Adam\"} \n",
        "\n",
        "hyperparams_dict = itertools.product(lr_dict, epochs, optim_dict, loss_f_dict)\n",
        "results = train_ptnet(hyperparams_dict) \n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}