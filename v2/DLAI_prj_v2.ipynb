{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "888aef9c-3e20-41d0-a295-177775933dc5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-05T15:09:28.540317Z",
     "iopub.status.busy": "2022-09-05T15:09:28.539241Z",
     "iopub.status.idle": "2022-09-05T15:09:28.545471Z",
     "shell.execute_reply": "2022-09-05T15:09:28.544257Z",
     "shell.execute_reply.started": "2022-09-05T15:09:28.540230Z"
    }
   },
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e8261e-9a73-42ab-8736-bdda11d8ce47",
   "metadata": {},
   "source": [
    "## Code attributions\n",
    "\n",
    "Pyg: \n",
    "- https://pytorch-geometric.readthedocs.io/en/latest/notes/colabs.html\n",
    "\n",
    "Wandb:\n",
    "- https://colab.research.google.com/github/wandb/examples/blob/master/colabs/pytorch/Simple_PyTorch_Integration.ipynb\n",
    "- https://colab.research.google.com/github/wandb/examples/blob/master/colabs/pytorch/Organizing_Hyperparameter_Sweeps_in_PyTorch_with_W%26B.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06ab59a-7b10-4cdb-82a3-50a319601939",
   "metadata": {},
   "source": [
    "## Dependencies and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d5ed4db-0713-4f52-952b-38866fab82d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-09T14:25:44.274964Z",
     "iopub.status.busy": "2022-09-09T14:25:44.274274Z",
     "iopub.status.idle": "2022-09-09T14:26:15.000073Z",
     "shell.execute_reply": "2022-09-09T14:26:14.998929Z",
     "shell.execute_reply.started": "2022-09-09T14:25:44.274889Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0+cu116\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting wandb\n",
      "  Downloading wandb-0.13.3-py2.py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m80.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting setproctitle\n",
      "  Downloading setproctitle-1.3.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
      "Collecting sentry-sdk>=1.0.0\n",
      "  Downloading sentry_sdk-1.9.8-py2.py3-none-any.whl (158 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.7/158.7 kB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pathtools\n",
      "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.9/dist-packages (from wandb) (5.4.1)\n",
      "Collecting promise<3,>=2.0\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from wandb) (63.1.0)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (3.1.27)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (8.1.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (5.9.1)\n",
      "Collecting shortuuid>=0.5.0\n",
      "  Downloading shortuuid-1.0.9-py3-none-any.whl (9.4 kB)\n",
      "Requirement already satisfied: six>=1.13.0 in /usr/lib/python3/dist-packages (from wandb) (1.14.0)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (2.28.1)\n",
      "Collecting docker-pycreds>=0.4.0\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Requirement already satisfied: protobuf<4.0dev,>=3.12.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (3.19.4)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.9/dist-packages (from GitPython>=1.0.0->wandb) (4.0.9)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (2.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3,>=2.0.0->wandb) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3,>=2.0.0->wandb) (2019.11.28)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.10)\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Downloading urllib3-1.26.12-py2.py3-none-any.whl (140 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.4/140.4 kB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.9/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n",
      "Building wheels for collected packages: promise, pathtools\n",
      "  Building wheel for promise (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21486 sha256=3bd51eee27353336741e0b2d27146fa36a1cfa2f12498dd13e68599530591945\n",
      "  Stored in directory: /root/.cache/pip/wheels/e1/e8/83/ddea66100678d139b14bc87692ece57c6a2a937956d2532608\n",
      "  Building wheel for pathtools (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=9a9646a21bab8e4c1da72b67eb76d419244e90db5bcdeaee3efecd7eca61cc8f\n",
      "  Stored in directory: /root/.cache/pip/wheels/b7/0a/67/ada2a22079218c75a88361c0782855cc72aebc4d18d0289d05\n",
      "Successfully built promise pathtools\n",
      "Installing collected packages: pathtools, urllib3, shortuuid, setproctitle, promise, docker-pycreds, sentry-sdk, wandb\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.10\n",
      "    Uninstalling urllib3-1.26.10:\n",
      "      Successfully uninstalled urllib3-1.26.10\n",
      "Successfully installed docker-pycreds-0.4.0 pathtools-0.1.2 promise-2.3 sentry-sdk-1.9.8 setproctitle-1.3.2 shortuuid-1.0.9 urllib3-1.26.12 wandb-0.13.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  ········································\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@title Dependencies\n",
    "\n",
    "import os\n",
    "import itertools\n",
    "import torch\n",
    "import copy\n",
    "from tqdm.notebook import tqdm\n",
    "os.environ['TORCH'] = torch.__version__\n",
    "print(torch.__version__)\n",
    "\n",
    "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "!pip install -q torch-cluster -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.manual_seed(12345)\n",
    "\n",
    "#wandb setup\n",
    "!pip install wandb\n",
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "676fc795-606a-464b-882e-3842b29d04fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-09T14:26:18.655859Z",
     "iopub.status.busy": "2022-09-09T14:26:18.655512Z",
     "iopub.status.idle": "2022-09-09T14:26:18.666442Z",
     "shell.execute_reply": "2022-09-09T14:26:18.665619Z",
     "shell.execute_reply.started": "2022-09-09T14:26:18.655834Z"
    }
   },
   "outputs": [],
   "source": [
    "#@title Utility functions\n",
    "from collections import defaultdict\n",
    "\n",
    "#Visualization methods for 3d shapes and point clouds\n",
    "def visualize_mesh(pos, face):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.gca(projection='3d')\n",
    "    ax.axes.xaxis.set_ticklabels([])\n",
    "    ax.axes.yaxis.set_ticklabels([])\n",
    "    ax.axes.zaxis.set_ticklabels([])\n",
    "    ax.plot_trisurf(pos[:, 0], pos[:, 1], pos[:, 2], triangles=face.t(), antialiased=False)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualize_points(pos, edge_index=None, index=None):\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    if edge_index is not None:\n",
    "        for (src, dst) in edge_index.t().tolist():\n",
    "             src = pos[src].tolist()\n",
    "             dst = pos[dst].tolist()\n",
    "             plt.plot([src[0], dst[0]], [src[1], dst[1]], linewidth=1, color='black')\n",
    "    if index is None:\n",
    "        plt.scatter(pos[:, 0], pos[:, 1], s=50, zorder=1000)\n",
    "    else:\n",
    "       mask = torch.zeros(pos.size(0), dtype=torch.bool)\n",
    "       mask[index] = True\n",
    "       plt.scatter(pos[~mask, 0], pos[~mask, 1], s=50, color='lightgray', zorder=1000)\n",
    "       plt.scatter(pos[mask, 0], pos[mask, 1], s=50, zorder=1000)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "#other utility methods for later\n",
    "def get_optimizer(opt_name, model, lr):\n",
    "  #There is no match in python 3.7, which is the version colab uses :/\n",
    "    if opt_name == \"Adam\":\n",
    "        return torch.optim.Adam(model.parameters(), lr)\n",
    "    elif opt_name == \"SGD\":\n",
    "        return torch.optim.SGD(model.parameters(), lr, momentum=1e-4,\n",
    "                           dampening=1e-6)\n",
    "    return None\n",
    "    \n",
    "def shrink_ModelNet(dataset, max):\n",
    "    datalist = []\n",
    "    num_classes = defaultdict(lambda: 0)\n",
    "    for data in dataset:\n",
    "        class_num = int(data[0].y)\n",
    "        if num_classes[class_num] < max:\n",
    "            datalist.append(data)\n",
    "            num_classes[class_num] += 1\n",
    "    \n",
    "    return datalist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "261f869c-7e7e-423c-94e9-fe220db70616",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-09T14:26:19.886388Z",
     "iopub.status.busy": "2022-09-09T14:26:19.886062Z",
     "iopub.status.idle": "2022-09-09T14:26:20.546767Z",
     "shell.execute_reply": "2022-09-09T14:26:20.546033Z",
     "shell.execute_reply.started": "2022-09-09T14:26:19.886364Z"
    }
   },
   "outputs": [],
   "source": [
    "#@title Additional configs\n",
    "from torch_geometric.transforms import RandomRotate, Compose, SamplePoints, ToDevice\n",
    "\n",
    "dataset = \"ModelNet\"\n",
    "\n",
    "if dataset==\"ModelNet\":\n",
    "    nr_classes=10\n",
    "    batch_size=100\n",
    "    max_item_per_class=101\n",
    "    nr_points=1024\n",
    "    training_ds_root = \"training_data_1\"\n",
    "    test_ds_root = \"test_data_1\"\n",
    "else:\n",
    "    nr_classes=40\n",
    "    batch_size=40\n",
    "    nr_points=256\n",
    "    max_item_per_class=1\n",
    "    training_ds_root = \"training_data\",\n",
    "    test_ds_root = \"test_data\"\n",
    "    \n",
    "#Augmentors and transformers for the data\n",
    "augmentor = Compose([\n",
    "    RandomRotate(degrees=90, axis=0),\n",
    "    RandomRotate(degrees=90, axis=1),\n",
    "    RandomRotate(degrees=90, axis=2)\n",
    "])\n",
    "\n",
    "transformer=Compose([\n",
    "    SamplePoints(num=nr_points, include_normals=True),\n",
    "    ToDevice(device)\n",
    "])\n",
    "\n",
    "#Create dictionary with hp \n",
    "config_contrastive=dict(\n",
    "    dataset = dataset,\n",
    "    max_item_per_class=max_item_per_class,\n",
    "    epochs=130,\n",
    "    classes=nr_classes,\n",
    "    batch_size=batch_size,\n",
    "    lr=0.01,\n",
    "    temperature=0.03,\n",
    "    optimizer=\"Adam\",\n",
    "    train_ds_root = training_ds_root,\n",
    "    test_ds_root = test_ds_root,\n",
    "    classifier_epochs=200,\n",
    "    classifier_lr = 0.05\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f63ab00-0c10-4e14-adee-753de2b1cc32",
   "metadata": {},
   "source": [
    "## Dataset and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6cb1db65-3fa5-4267-8e14-a587bc2d0da4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-09T14:26:20.951224Z",
     "iopub.status.busy": "2022-09-09T14:26:20.950887Z",
     "iopub.status.idle": "2022-09-09T14:26:20.978207Z",
     "shell.execute_reply": "2022-09-09T14:26:20.977531Z",
     "shell.execute_reply.started": "2022-09-09T14:26:20.951199Z"
    }
   },
   "outputs": [],
   "source": [
    "#@title Extend dataset with augmented samples\n",
    "\n",
    "from torch_geometric.data import Dataset\n",
    "from torch_geometric.datasets import GeometricShapes, ModelNet\n",
    "from torch_geometric.loader import DataLoader\n",
    "  \n",
    "#Extend existing Class\n",
    "class AugmentedDS(Dataset):\n",
    "\n",
    "  \"\"\"An augmented version of the GeometricShapes dataset\"\"\"\n",
    "\n",
    "  def __init__(self, root: str, train:bool, augmentor, transformer, ds_name: str):\n",
    "        \n",
    "        if ds_name == \"GeometricShapes\":\n",
    "            self.dataset = GeometricShapes(root = root, train = train)\n",
    "        else:\n",
    "            self.dataset = ModelNet(root=root, train=train, name='10')\n",
    "        self.transformer = transformer\n",
    "        self.augmentor_transformer = Compose([augmentor,transformer])\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.dataset)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "\n",
    "    original_shape = self.dataset[idx]\n",
    "    augmented_shape1 = self.augmentor_transformer(original_shape.clone()) \n",
    "    augmented_shape2 = self.augmentor_transformer(original_shape.clone())\n",
    "    augmented_shape3 = self.augmentor_transformer(original_shape.clone()) \n",
    "    original_shape = self.transformer(original_shape)\n",
    "\n",
    "    return original_shape, augmented_shape1, augmented_shape2, augmented_shape3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb7dd602-4f87-4e1c-8622-81c77d0819a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-09T14:26:22.369982Z",
     "iopub.status.busy": "2022-09-09T14:26:22.369463Z",
     "iopub.status.idle": "2022-09-09T14:26:22.481722Z",
     "shell.execute_reply": "2022-09-09T14:26:22.481019Z",
     "shell.execute_reply.started": "2022-09-09T14:26:22.369952Z"
    }
   },
   "outputs": [],
   "source": [
    "#@title Create simple PPFnet\n",
    "\n",
    "from torch_geometric.nn import PPFConv, global_max_pool\n",
    "from torch.nn import Sequential, Linear, ReLU\n",
    "from torch_cluster import fps, knn_graph\n",
    "\n",
    "class PPFNet(torch.nn.Module):\n",
    "    def __init__(self, feat_dim, hidden_layer_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        #Dimension of hidden layers\n",
    "        self.hidden_layer_dim = hidden_layer_dim\n",
    "        #Dimension of feature, 3 with 3D fd and 4 with 4D\n",
    "        self.feat_dim = feat_dim\n",
    "\n",
    "        \n",
    "        mlp1 = Sequential(Linear(self.feat_dim, self.hidden_layer_dim),\n",
    "                              ReLU(),\n",
    "                              Linear(self.hidden_layer_dim, self.hidden_layer_dim*2))\n",
    "        self.conv1 = PPFConv(local_nn = mlp1)  \n",
    "\n",
    "        \n",
    "        mlp2 = Sequential(Linear(self.hidden_layer_dim*2 + self.feat_dim, self.hidden_layer_dim*2 + self.feat_dim),\n",
    "                              ReLU(),\n",
    "                              Linear(self.hidden_layer_dim*2 + self.feat_dim, self.hidden_layer_dim*2 + self.feat_dim))  \n",
    "        self.conv2 = PPFConv(local_nn = mlp2)  \n",
    "        \n",
    "        self.prj_head = Sequential(\n",
    "            Linear(self.hidden_layer_dim*2 + self.feat_dim, 100),\n",
    "            ReLU(),\n",
    "            Linear(100,100)\n",
    "        )\n",
    "        \n",
    "    def forward(self, pos, normal, batch):\n",
    "\n",
    "        #Create edges in the point cloud\n",
    "        edge_index = knn_graph(pos, k=16, batch=batch, loop=False)\n",
    "        \n",
    "        #There are no features in first convolution\n",
    "        #Other datasets different from GS or MN may have them\n",
    "        x = self.conv1(x=None, pos=pos, normal=normal, edge_index=edge_index)\n",
    "        x = x.relu()\n",
    "        \n",
    "        if True:\n",
    "            #farthest point sampling\n",
    "            index = fps(pos, batch, ratio=0.5)\n",
    "            x = x[index]\n",
    "            pos = pos[index]\n",
    "            normal = normal[index]\n",
    "            batch = batch[index]\n",
    "            edge_index = knn_graph(pos, k=16, batch=batch, loop=False)\n",
    "        \n",
    "        x = self.conv2(x=x, pos=pos, normal=normal, edge_index=edge_index)\n",
    "        x = x.relu()\n",
    "        \n",
    "        x = global_max_pool(x, batch)  # [num_examples, hidden_channels]\n",
    "        return self.prj_head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65084d1d-3de4-4a6b-938c-b4e5cfed81c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-09T14:26:24.093992Z",
     "iopub.status.busy": "2022-09-09T14:26:24.093654Z",
     "iopub.status.idle": "2022-09-09T14:26:24.100977Z",
     "shell.execute_reply": "2022-09-09T14:26:24.100301Z",
     "shell.execute_reply.started": "2022-09-09T14:26:24.093967Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_contrastive(config):\n",
    "    # Make the data\n",
    "    train_ds = AugmentedDS(root = config.train_ds_root, augmentor=augmentor,\n",
    "                       transformer=transformer, train=True, ds_name = config.dataset)\n",
    "    test_ds = AugmentedDS(root = config.test_ds_root, augmentor=augmentor,\n",
    "                       transformer=transformer, train=False, ds_name = config.dataset)\n",
    "    \n",
    "    #Shrink dataset if it's modelnet\n",
    "    if dataset == \"ModelNet\":\n",
    "        datalist_train = shrink_ModelNet(train_ds, config.max_item_per_class)\n",
    "        datalist_test = shrink_ModelNet(test_ds, config.max_item_per_class)\n",
    "\n",
    "        train_dl = DataLoader(datalist_train, batch_size = config.batch_size, shuffle=True)\n",
    "        test_dl = DataLoader(datalist_test, batch_size = config.batch_size, shuffle=True)\n",
    "    else:\n",
    "        train_dl = DataLoader(train_ds, batch_size = config.batch_size, shuffle=True)\n",
    "        test_dl = DataLoader(test_ds, batch_size = config.batch_size, shuffle=True) \n",
    "    \n",
    "    # Make the model\n",
    "    model = PPFNet(feat_dim=4, hidden_layer_dim = 32).to(device)\n",
    "\n",
    "    # Make optimizer\n",
    "    optimizer = get_optimizer(opt_name = config.optimizer, model = model, lr=config.lr)\n",
    "    \n",
    "    return model, train_dl, test_dl, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8073fb9-74a8-45bd-a4b4-55916651d5bd",
   "metadata": {},
   "source": [
    "## Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0c48db6-d7ec-45c0-bd1b-efe635734db6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-09T14:26:25.368326Z",
     "iopub.status.busy": "2022-09-09T14:26:25.367990Z",
     "iopub.status.idle": "2022-09-09T14:26:25.380060Z",
     "shell.execute_reply": "2022-09-09T14:26:25.379408Z",
     "shell.execute_reply.started": "2022-09-09T14:26:25.368302Z"
    }
   },
   "outputs": [],
   "source": [
    "#@title InfoNCE loss implementations\n",
    "from torch.nn import CosineSimilarity\n",
    "\n",
    "cos = CosineSimilarity(dim=0)\n",
    "cos2 = CosineSimilarity(dim=1)\n",
    "\n",
    "def InfoNCELossSN(anchors, augmented, temperature=0.05):\n",
    "\n",
    "  loss = 0\n",
    "  batch_len = anchors.shape[0]\n",
    "\n",
    "  for index in range(batch_len):\n",
    "    anchor = anchors[index]\n",
    "    pos_sample = augmented[index]\n",
    "    pos_sim = torch.exp(cos(anchor, pos_sample) / temperature)\n",
    "    neg_sim = 0\n",
    "    \n",
    "    if index == 0:\n",
    "        negatives = augmented[index+1:, :]\n",
    "    elif index == batch_len-1:\n",
    "        negatives = augmented[:index, :]\n",
    "    else:\n",
    "        negatives = torch.cat((augmented[:index, :], augmented[index+1:, :]), dim=0)\n",
    "    \n",
    "    neg_sim = torch.einsum(\"i-> \", torch.exp(cos2(anchor, negatives) / temperature))\n",
    "        \n",
    "    loss += -torch.log(pos_sim / (pos_sim + neg_sim))\n",
    "    \n",
    "  return loss \n",
    "\n",
    "def InfoNCELoss(anchors, augmented1, augmented2, temperature=0.05):\n",
    "\n",
    "  \"\"\"InfoNCE with support to multiple positives\"\"\"\n",
    "  loss = 0\n",
    "  batch_len = anchors.shape[0]\n",
    "\n",
    "  for index in range(batch_len):\n",
    "    anchor = anchors[index]\n",
    "    positives = (torch.cat((augmented1[index], augmented2[index]), dim=0)).reshape(2, -1)\n",
    "    pos_sim = torch.einsum(\"i->\",torch.exp(cos2(anchor, positives) / temperature))\n",
    "    \n",
    "    neg_sim = 0\n",
    "    #Handle first and last row\n",
    "    if index == 0:\n",
    "        negatives = torch.cat((augmented1[index+1:, :], augmented2[index+1:, :]), dim=0)\n",
    "    elif index == batch_len-1:\n",
    "        negatives = torch.cat((augmented1[:index, :],augmented2[:index, :]),dim=0)\n",
    "    else:\n",
    "        n1 = torch.cat((augmented1[:index, :], augmented1[index+1:, :]), dim=0)\n",
    "        n2 = torch.cat((augmented2[:index, :], augmented2[index+1:, :]), dim=0)\n",
    "        negatives = torch.cat((n1, n2), dim=0)\n",
    "         \n",
    "    neg_sim = torch.einsum(\"i-> \", torch.exp(cos2(anchor, negatives) / temperature))\n",
    "    \n",
    "    loss += -torch.log(pos_sim / (pos_sim + neg_sim))\n",
    "\n",
    "  return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6cda24ae-2e6d-4292-967c-dfe3161afeb8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-09T14:26:28.288474Z",
     "iopub.status.busy": "2022-09-09T14:26:28.288145Z",
     "iopub.status.idle": "2022-09-09T14:26:28.295521Z",
     "shell.execute_reply": "2022-09-09T14:26:28.294650Z",
     "shell.execute_reply.started": "2022-09-09T14:26:28.288448Z"
    }
   },
   "outputs": [],
   "source": [
    "#@title Training functions\n",
    "\n",
    "def train_batch(model, optim, loader, temperature, scheduler=None):\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    for data in loader:\n",
    "        optim.zero_grad()  # Clear gradients.\n",
    "        \n",
    "        #encode original and augmentations\n",
    "        original, augmented1, augmented2 = data[0], data[1], data[2]\n",
    "        z1 = model(original.pos, original.normal, original.batch)\n",
    "        z2 = model(augmented1.pos, augmented1.normal, augmented1.batch)\n",
    "        z3 = model(augmented2.pos, augmented2.normal, augmented2.batch)\n",
    "\n",
    "        loss = InfoNCELoss(anchors=z1, augmented1=z2, augmented2=z3, \n",
    "                            temperature=temperature)\n",
    "        #loss = InfoNCELossSN(anchors=z1, augmented=z2, temperature=temperature)\n",
    "                            \n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        total_loss += loss\n",
    "\n",
    "    scheduler.step() if scheduler is not None else None\n",
    "    return total_loss                 \n",
    "\n",
    "def train_log(loss, epoch):\n",
    "    # log to wandb\n",
    "    wandb.log({\"loss\": loss}, step=epoch)\n",
    "    \n",
    "    \n",
    "def train(model, loader, optimizer, config):\n",
    "    # Tell wandb to watch what the model gets up to: gradients, weights, and more!\n",
    "    wandb.watch(model, log=\"all\", log_freq=10)\n",
    "    \n",
    "    for epoch in tqdm(range(config.epochs)):\n",
    "        loss = train_batch(model, optimizer, loader, config.temperature)\n",
    "        train_log(loss, epoch)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d630d05a-302d-46cf-8cfa-390a78f531e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-09T14:28:29.662405Z",
     "iopub.status.busy": "2022-09-09T14:28:29.661595Z",
     "iopub.status.idle": "2022-09-09T14:28:29.671138Z",
     "shell.execute_reply": "2022-09-09T14:28:29.670607Z",
     "shell.execute_reply.started": "2022-09-09T14:28:29.662372Z"
    }
   },
   "outputs": [],
   "source": [
    "#@title Test the result of contrastive training\n",
    "#Note that the network has to be trained in order to be used for testing\n",
    "\n",
    "class SimpleClassifier(torch.nn.Module):\n",
    "\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "    self.mlp = torch.nn.Linear(in_features=100, out_features=10)\n",
    "    self.activation = torch.nn.ReLU()\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.activation(self.mlp(x))\n",
    "\n",
    "\n",
    "def train_classifier(model, classifier, optim, loader, scheduler=None):\n",
    "    classifier.train()\n",
    "    \n",
    "    total_loss = 0\n",
    "    loss_f = torch.nn.CrossEntropyLoss()\n",
    "    for data in loader:\n",
    "\n",
    "        optim.zero_grad()  # Clear gradients.\n",
    "        original = data[0]\n",
    "        z1 = model(original.pos, original.normal, original.batch)\n",
    "        \n",
    "        logits = classifier(z1)\n",
    "        loss = loss_f(logits, data[0].y)\n",
    "        loss.backward()  # Backward pass.\n",
    "        \n",
    "        optim.step()  # Update model parameters.\n",
    "        total_loss += loss\n",
    "\n",
    "    scheduler.step() if scheduler is not None else None\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test_classifier(model, classifier, loader):\n",
    "    classifier.eval()\n",
    "\n",
    "    total_correct = 0\n",
    "    for data in loader:\n",
    "\n",
    "        original = data[0]\n",
    "        z1 = model(original.pos, original.normal, original.batch)\n",
    "        logits = classifier(z1)\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "        total_correct += int((preds == data[0].y).sum())\n",
    "\n",
    "    return total_correct/(len(loader.dataset))\n",
    "\n",
    "def test(model, train_loader, test_loader, config):\n",
    "    # Run the model on some test examples\n",
    "    \n",
    "    classifier = SimpleClassifier().to(device)\n",
    "    optimizer = get_optimizer(opt_name = config.optimizer, model = classifier, lr=0.01)\n",
    "    wandb.watch(classifier, log=\"all\", log_freq=10)\n",
    "    \n",
    "    for epoch in tqdm(range(config.classifier_epochs)):\n",
    "        loss = train_classifier(model, classifier, optimizer, train_loader)\n",
    "        wandb.log({\"classifier train loss\": loss}, step=epoch)   \n",
    "        \n",
    "        test_acc = test_classifier(model, classifier, test_loader)\n",
    "        wandb.log({\"classifier test accuracy\": test_acc}, step=epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2fba8fd1-f713-4f69-890b-b3c3639a2ea5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-09T14:28:30.860583Z",
     "iopub.status.busy": "2022-09-09T14:28:30.859775Z",
     "iopub.status.idle": "2022-09-09T14:28:30.864309Z",
     "shell.execute_reply": "2022-09-09T14:28:30.863824Z",
     "shell.execute_reply.started": "2022-09-09T14:28:30.860551Z"
    }
   },
   "outputs": [],
   "source": [
    "def model_pipeline(hyperparameters):\n",
    "    with wandb.init(project=\"contrastive_training\", config=hyperparameters):\n",
    "        config = wandb.config\n",
    "        # make the model, data, and optimization problem\n",
    "        model, train_loader, test_loader, optimizer = make_contrastive(config)\n",
    "        print(model)\n",
    "\n",
    "        #and use them to train the model\n",
    "        train(model, train_loader, optimizer, config)\n",
    "\n",
    "        # and test its final performance\n",
    "        test(model, train_loader, test_loader, config)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b2ae260-c1c8-4dd4-8040-ab535127b096",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-09T14:28:32.086151Z",
     "iopub.status.busy": "2022-09-09T14:28:32.085359Z",
     "iopub.status.idle": "2022-09-09T15:04:22.248748Z",
     "shell.execute_reply": "2022-09-09T15:04:22.247857Z",
     "shell.execute_reply.started": "2022-09-09T14:28:32.086119Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmattewg_dev\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/notebooks/wandb/run-20220909_142832-d0cj88dc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/mattewg_dev/contrastive_training/runs/d0cj88dc\" target=\"_blank\">super-sea-19</a></strong> to <a href=\"https://wandb.ai/mattewg_dev/contrastive_training\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPFNet(\n",
      "  (conv1): PPFConv(local_nn=Sequential(\n",
      "    (0): Linear(in_features=4, out_features=32, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=32, out_features=64, bias=True)\n",
      "  ), global_nn=None)\n",
      "  (conv2): PPFConv(local_nn=Sequential(\n",
      "    (0): Linear(in_features=68, out_features=68, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=68, out_features=68, bias=True)\n",
      "  ), global_nn=None)\n",
      "  (prj_head): Sequential(\n",
      "    (0): Linear(in_features=68, out_features=100, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21848eb084154cf5bbcb32d859262d8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/130 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30611603f6c84e4abcbe8b01603c6f1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca8610aa86af489cb95324c8835d17c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>classifier test accuracy</td><td>▃█▆▃▅▃▄▂▂▃▄▁▃█▄▄▁▅▆▅▇▂▅▅▄▃▄▆▃▆▃█▆▁▂▃▇▄▆▂</td></tr><tr><td>classifier train loss</td><td>▄▄▆▄▅▅▆▅▄▄▃▇▆▄▃▃▄▃▁█▆▄▃▇▅▄▅▅▅▅▂▃▃▅▁▄▁▆▄▇</td></tr><tr><td>loss</td><td>█▅▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>classifier test accuracy</td><td>0.57269</td></tr><tr><td>classifier train loss</td><td>0.01287</td></tr><tr><td>loss</td><td>63.65488</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">super-sea-19</strong>: <a href=\"https://wandb.ai/mattewg_dev/contrastive_training/runs/d0cj88dc\" target=\"_blank\">https://wandb.ai/mattewg_dev/contrastive_training/runs/d0cj88dc</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220909_142832-d0cj88dc/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#@title Run pipeline\n",
    "\n",
    "model = model_pipeline(config_contrastive)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
